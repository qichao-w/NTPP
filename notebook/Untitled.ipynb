{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a599db00-22dd-46ca-99c9-6ebfde65dc5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:33:36.359680Z",
     "iopub.status.busy": "2024-09-22T08:33:36.359513Z",
     "iopub.status.idle": "2024-09-22T08:33:39.531016Z",
     "shell.execute_reply": "2024-09-22T08:33:39.530333Z",
     "shell.execute_reply.started": "2024-09-22T08:33:36.359667Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers.modeling_flash_attention_utils import _flash_attention_forward\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers import ROPE_INIT_FUNCTIONS\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    is_torchdynamo_compiling,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers import LlamaConfig\n",
    "\n",
    "from transformers import LlamaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b0ef9f-c34c-40f0-af6f-a41ed5dc7891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:33:39.532335Z",
     "iopub.status.busy": "2024-09-22T08:33:39.531951Z",
     "iopub.status.idle": "2024-09-22T08:33:39.540035Z",
     "shell.execute_reply": "2024-09-22T08:33:39.539432Z",
     "shell.execute_reply.started": "2024-09-22T08:33:39.532320Z"
    }
   },
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*):\n",
    "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
    "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence\n",
    "            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n",
    "                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n",
    "                with `head_dim` being the embedding dimension of each attention head.\n",
    "            kwargs (`dict`, *optional*):\n",
    "                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n",
    "                into the model\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97838453-34da-4504-8f30-dea3449c2939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:33:39.541093Z",
     "iopub.status.busy": "2024-09-22T08:33:39.540816Z",
     "iopub.status.idle": "2024-09-22T08:33:39.556596Z",
     "shell.execute_reply": "2024-09-22T08:33:39.556045Z",
     "shell.execute_reply.started": "2024-09-22T08:33:39.541079Z"
    }
   },
   "outputs": [],
   "source": [
    "class LlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        return_legacy_cache = False\n",
    "        if (\n",
    "            use_cache and not isinstance(past_key_values, Cache) and not self.training\n",
    "        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "            return_legacy_cache = True\n",
    "            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "            logger.warning_once(\n",
    "                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n",
    "                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n",
    "            )\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                    position_embeddings,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n",
    "        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n",
    "        # to infer the attention mask.\n",
    "        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        using_static_cache = isinstance(past_key_values, StaticCache)\n",
    "\n",
    "        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n",
    "        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n",
    "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
    "                attention_mask,\n",
    "                inputs_embeds=input_tensor,\n",
    "                past_key_values_length=past_seen_tokens,\n",
    "                is_training=self.training,\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if using_static_cache:\n",
    "            target_length = past_key_values.get_max_length()\n",
    "        else:\n",
    "            target_length = (\n",
    "                attention_mask.shape[-1]\n",
    "                if isinstance(attention_mask, torch.Tensor)\n",
    "                else past_seen_tokens + sequence_length + 1\n",
    "            )\n",
    "\n",
    "        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n",
    "        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=sequence_length,\n",
    "            target_length=target_length,\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "            min_dtype=min_dtype,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and attention_mask is not None\n",
    "            and attention_mask.device.type == \"cuda\"\n",
    "            and not output_attentions\n",
    "        ):\n",
    "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
    "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
    "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
    "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
    "\n",
    "        return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9d62c3-ed77-45a0-9526-ffaf6227de97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:33:39.557586Z",
     "iopub.status.busy": "2024-09-22T08:33:39.557442Z",
     "iopub.status.idle": "2024-09-22T08:33:39.573383Z",
     "shell.execute_reply": "2024-09-22T08:33:39.572797Z",
     "shell.execute_reply.started": "2024-09-22T08:33:39.557574Z"
    }
   },
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            if labels is None and not is_torchdynamo_compiling():\n",
    "                logger.warning_once(\n",
    "                    \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n",
    "                )\n",
    "            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "            # TODO: remove the float() operation in v4.46\n",
    "            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        cache_position=None,\n",
    "        position_ids=None,\n",
    "        use_cache=True,\n",
    "        num_logits_to_keep=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n",
    "        # Exception 1: when passing input_embeds, input_ids may be missing entries\n",
    "        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n",
    "        if past_key_values is not None:\n",
    "            if inputs_embeds is not None:  # Exception 1\n",
    "                input_ids = input_ids[:, -cache_position.shape[0] :]\n",
    "            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n",
    "                input_ids = input_ids[:, cache_position]\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n",
    "                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and cache_position[0] == 0:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n",
    "        else:\n",
    "            # The clone here is for the same reason as for `position_ids`.\n",
    "            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n",
    "\n",
    "        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n",
    "            if model_inputs[\"inputs_embeds\"] is not None:\n",
    "                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n",
    "                device = model_inputs[\"inputs_embeds\"].device\n",
    "            else:\n",
    "                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n",
    "                device = model_inputs[\"input_ids\"].device\n",
    "\n",
    "            dtype = self.lm_head.weight.dtype\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "\n",
    "            attention_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "                attention_mask,\n",
    "                sequence_length=sequence_length,\n",
    "                target_length=past_key_values.get_max_length(),\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "                min_dtype=min_dtype,\n",
    "                cache_position=cache_position,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "        if num_logits_to_keep is not None:\n",
    "            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": use_cache,\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8a8488-9f70-40a6-9889-7e2dc7cbb53e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:33:39.574388Z",
     "iopub.status.busy": "2024-09-22T08:33:39.574081Z",
     "iopub.status.idle": "2024-09-22T08:33:40.774662Z",
     "shell.execute_reply": "2024-09-22T08:33:40.773146Z",
     "shell.execute_reply.started": "2024-09-22T08:33:39.574374Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLAMA_ATTENTION_CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/apdcephfs_qy3/share_1594716/bingzhe/pretrained/Llama-2-7b-hf/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3832\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3827\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3828\u001b[0m )\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3835\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m---> 16\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m LlamaRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m---> 16\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m LlamaRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mLLAMA_ATTENTION_CLASSES\u001b[49m[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLAMA_ATTENTION_CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"/apdcephfs_qy3/share_1594716/bingzhe/pretrained/Llama-2-7b-hf/\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706f955-1ae8-4681-b562-300c7631f89e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.775215Z",
     "iopub.status.idle": "2024-09-22T08:33:40.775427Z",
     "shell.execute_reply": "2024-09-22T08:33:40.775334Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.775325Z"
    }
   },
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.rotary_emb.original_max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fef612-d9c8-4280-9bde-ec93fc0a2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fa271-63d9-4ede-9357-acf7e8e927f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38ee1f-f1aa-4503-a47f-6aeed277d349",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.776307Z",
     "iopub.status.idle": "2024-09-22T08:33:40.776502Z",
     "shell.execute_reply": "2024-09-22T08:33:40.776410Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.776402Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    hidden_size: int = 100\n",
    "    vocab_size: int = 20\n",
    "    pretraining_tp : int = 4\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "lm_head = nn.Linear(in_features=config.hidden_size, out_features=config.vocab_size, bias=False)\n",
    "\n",
    "# 生成input 数据\n",
    "input_x = torch.randn(3, 100)\n",
    "\n",
    "\n",
    "# 正常计算\n",
    "normal_value = lm_head(input_x)\n",
    "normal_value.shape\n",
    "\n",
    "# TP计算\n",
    "lm_head_slices = lm_head.weight.split(config.vocab_size // config.pretraining_tp, dim=0)\n",
    "[i.shape for i in lm_head_slices]\n",
    "\n",
    "\n",
    "tp_value = torch.cat([F.linear(input_x, lm_head_slices[i]) for i in range(4)], dim=-1)\n",
    "tp_value.shape\n",
    "\n",
    "# 检查两个结果是不是一样\n",
    "torch.allclose(normal_value, tp_value)\n",
    "# torch.testing.assert_close(normal_value, tp_value)AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4822e8-1107-4a2c-91d8-737b853f4141",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.777414Z",
     "iopub.status.idle": "2024-09-22T08:33:40.777605Z",
     "shell.execute_reply": "2024-09-22T08:33:40.777511Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.777503Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n",
    "if sequence_length != 1:\n",
    "    causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "if attention_mask is not None:\n",
    "    causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "    mask_length = attention_mask.shape[-1]\n",
    "    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "    padding_mask = padding_mask == 0\n",
    "    causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "        padding_mask, min_dtype\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0b4d1-36b1-493a-a409-43bfd9d6f267",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.778154Z",
     "iopub.status.idle": "2024-09-22T08:33:40.778343Z",
     "shell.execute_reply": "2024-09-22T08:33:40.778246Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.778239Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 10)\n",
    "\n",
    "torch.repeat_interleave(x, repeats=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d45e61-3de4-498a-a247-cd106e896cfb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.779054Z",
     "iopub.status.idle": "2024-09-22T08:33:40.779259Z",
     "shell.execute_reply": "2024-09-22T08:33:40.779147Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.779139Z"
    }
   },
   "outputs": [],
   "source": [
    "net = torch.nn.Embedding(2, 128)\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randint(0, 2, size=(2, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce810143-9ce7-4eb6-b63a-996270b212f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:38:35.401411Z",
     "iopub.status.busy": "2024-09-22T08:38:35.400879Z",
     "iopub.status.idle": "2024-09-22T08:38:35.407125Z",
     "shell.execute_reply": "2024-09-22T08:38:35.406617Z",
     "shell.execute_reply.started": "2024-09-22T08:38:35.401394Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.arange(30) % 2\n",
    "\n",
    "x = x.expand(2, 32, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9277a553-d1fd-46be-88fc-291d250d8efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:38:42.554751Z",
     "iopub.status.busy": "2024-09-22T08:38:42.554413Z",
     "iopub.status.idle": "2024-09-22T08:38:42.559257Z",
     "shell.execute_reply": "2024-09-22T08:38:42.558774Z",
     "shell.execute_reply.started": "2024-09-22T08:38:42.554734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab8b84b-f7a4-4e75-83b5-1fd55cfd2a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:38:54.628348Z",
     "iopub.status.busy": "2024-09-22T08:38:54.627925Z",
     "iopub.status.idle": "2024-09-22T08:38:54.632914Z",
     "shell.execute_reply": "2024-09-22T08:38:54.632434Z",
     "shell.execute_reply.started": "2024-09-22T08:38:54.628326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0:32:2, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c46760-d457-4861-8b55-8366019cbb08",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.780619Z",
     "iopub.status.idle": "2024-09-22T08:33:40.780791Z",
     "shell.execute_reply": "2024-09-22T08:33:40.780712Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.780705Z"
    }
   },
   "outputs": [],
   "source": [
    "causal_mask = torch.randint(0, 100, (2, 32, 30, 30))\n",
    "\n",
    "idxs = torch.arange(1, causal_mask.size(-2), 2)\n",
    "\n",
    "causal_mask[:, :, idxs, idxs-1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8cae3-e9aa-4a2f-bc50-1eb68cbc826d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T08:33:40.781340Z",
     "iopub.status.idle": "2024-09-22T08:33:40.781513Z",
     "shell.execute_reply": "2024-09-22T08:33:40.781431Z",
     "shell.execute_reply.started": "2024-09-22T08:33:40.781424Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = torch.load(\"mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e1ac9b-50d3-4b17-9e25-e18fda7c4c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T08:54:44.724344Z",
     "iopub.status.busy": "2024-09-22T08:54:44.724047Z",
     "iopub.status.idle": "2024-09-22T08:54:44.727167Z",
     "shell.execute_reply": "2024-09-22T08:54:44.726722Z",
     "shell.execute_reply.started": "2024-09-22T08:54:44.724328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "for channel_idx in range(-5, 0):\n",
    "    print(channel_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63067502-2191-4635-9838-46d5e909c7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
